# Introduction
- 신경망 학습 방법은 실제 값, 이산 값, vector 값 목표 함수의 근사치에 대한 접근 방식을 제공
- 데이터 해석 학습과 같은 유형의 문제에 대해 가장 효과적인 방법 중 하나

## Biological Motivation
- 생물학점 학습 시스템이 상호 연결된 뉴런의 매우 복잡한 웹으로 구축되어 있다는 관찰에서 부분적으로 영감을 받음
- 인공신경망도 조밀하게 상호 연결된 단위 집합으로 구축됨
- 각 단위는 다수의 실제 값 입력과 실제 값을 출력


## Appropriate problems for neural network learning
- 역전파 알고리즘은 가장 일반적으로 사용되는 ANN 학습 기술
- 목표 함수 출력은 이산 값, 실수 값 또는 여러 실수 값 속성의 vector
- Network train algorithm > Decision Tree. 훈련 시간은 network의 가중치 수 고려된 훈련 예제의 수 및 다양한 학습 알고리즘 매개 변수의 설정과 같은 요인에 따라 몇 초에서 몇 시간까지 다양할 수 있음
- 학습된 목표 함수의 빠른 평가가 필요할 수 있음. ANN 학습 시간은 비교적 길지만, 학습된 신경망을 실제에 적용하기 위해 평가하는 것은 일반적으로 매우 빠름
- 학습된 신경망은 학습된 규칙보다 인간에게 쉽게 전달되지 않음

## Perceptron
- ANN system의 한 유형은 perceptron이라는 단위를 기반으로 함
- Perceptron은 실제 값 입력 벡터를 취하고, 이러한 입력의 선형 조합을 계산한 다음, 결과가 일부 임계값보다 크면 1을 출력하고, 그렇지 않으면 -1을 출력

<img width="255" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189894789-42b053df-c723-4636-a93d-22847637556b.PNG">

- 입력 $x_1$부터 x까지 perceptron에 의해 계산되는 출력은 다음과 같음

<img width="239" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189895033-422bb060-c45a-40fa-9e37-1b8cca1e0d36.PNG">

<img width="210" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189895534-fa73a0c3-316f-461f-b2b4-9090f2748268.PNG">

- Perceptron 학습에서 고려된 후보 가설 공간 H는 가능한 모든 실제 값 가중치 vecor의 집합

<img width="90" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189896191-81585925-0cd6-4995-9730-5ab16a8f5539.PNG">

### Representatinal Power of Perceptrons
- Perceptron을 n차원 공간에서 hyperplane에 나타나는 것

<img width="220" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189896529-58dbba7e-2827-4571-a11e-e9e2737eaf3d.PNG">

### The Perceptron Training Rule
- Perceptron 각 주어진 학습 data에 대해 올바른 출력을 생성하도록 하는 가중치 vector를 결정
- Perceptron 규칙과 Delta 규칙이라는 두 가지 고려
- 가중치 vector를 학습하는 한 가지 방법은 무작위 가중치로 시작한 다음 각 학습 data에 perceptron을 반복적으로 적용하여 가중치를 수정
- 위의 과정이 반복되며, perceptron이 모든 학습 data를 올바르게 분류할 때까지 필요한 만큼 훈련 예제를 반복

<img width="80" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189898786-0d84f2d1-ca53-497d-b13c-a7eaf931b32a.PNG">

- t : 현재 학습 과정에서의 target output, o : output generated by the perceptron, $\eta$ : 학습율(각 단계에서 가중치가 바뀌는 정도를 조절, 일반적으로 작은 값으로 설정)
- 목표 출력이 +1일 때 perceptron이 -1을 출력한다고 가정하였을 때, 이 경우 perceptron 출력을 -1이 아닌 +1로 만들려면 가중치를 변경하여 $\vec{w} \cdot \vec{x}$의 값을 증가시켜야 함

# Gradient Descent and the Delta Rule
- Perceptron 규칙은 학습 data가 선형적으로 분리될 수 있을 때 성공적인 가중치 vector를 찾지만, 학습 데이터가 선형적으로 분리되지 않으면 수렴하지 못할 수도 있음.
- Delta Rule은 위의 문제점을 해결하기 위해 등장
- Key idea : 경사하강법을 이용해서 가능한 가중치 vector의 가설 공간을 검색하여 학습 data에 적합한 가중치를 찾는 것
- 역전파의 기초
- 임계값이 없는 perceptron을 훈련시키는 작업
- 선형 단위로부터 가중치를 배우기 위해 training error를 측정하는 방법을 구체화
- 많은 error 측정 방법이 있지만, 가장 공통정인 방법은 아래와 같음

<img width="115" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189902841-258f2da6-9d25-4a9a-8b87-a7be9578b48d.PNG">

- D : training dataset, $t_d$ : training example의 target output, $o_d$ : linear unit의 output
- E는 $t_d$와 $o_d$ 차이의 제곱의 반이며, 모든 훈련 예제에 걸쳐 합계됨
- E는 $\vec{w}$의 함수 <- o는 가중치 vector에 의존

### VISUALIZING THE HYPOTHESIS SPACE

<img width="355" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189903742-1d829bea-16d8-4784-9670-f7ac1cdf6628.PNG">

- 축 $w_o$와 $w_1$은 단순한 선형 단위의 두 가중치에 대해 가능한 값 -> 전체 가설 공간을 나타냄
- 수직축 : 일부 고정된 훈련과 관련된 오류 E
- E를 정의하기 위한 방법을 고려할때, 선형 단위의 경우 이 오류 표면은 항상 전역 최소값이 되어야 함
- 경사하강법은 임의의 초기 가중치 vector로 시작한 다음 작은 단계로 반복적으로 수정하여 E를 최소화하는 가중치 vector를 결정
- 위의 과정은 global minimum에 도달할 때까지 계속

### DERIVATION OF THE GRADIENT DESCENT RULE
- 가장 가파른 하강 방향을 계산하기 위해서는 vector w의 각 성분에 대한 E의 도함수를 계산
- 이러한 vector 도함수를 vector w에 대한 E의 기울기라고 함.

<img width="133" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189904558-51d34359-b5b3-474d-85be-d51e52b5cb70.PNG">

- $\bigtriangledown E(\vec{w})$ 그 자체가 vector, 그 성분들은 각각의 원소에 대한 E의 부분 도함수
- 기울기는 E에서 가장 가파른 증가를 생성하는 방향을 지정 -> 이 vector의 음수는 가장 가파른 감소 방향 제시
- 경사하강 방법

![render](https://user-images.githubusercontent.com/80622859/189905670-3c58f6aa-b130-4db3-a957-36dd5995083c.png)

- $\eta$ : 학습 속도라고 불리는 양의 상수, 경사하강 단계에서 크기를 결정
- 가중치를 반복적으로 update하는 algorithm

<img width="150" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189905870-abf66f16-a2c0-48d8-b14a-d909ffb03e27.PNG">

- $\eta$가 작으면 algorithm은 훈련 예제가 선형적으로 분리 가능한지 여부에 관계없이 최소 오류가 있는 가중치로 수렴.
- $\eta$가 너무 크면 경사하강법은 오류 지표면에 도달하기 보다는 최소값을 통과할 가능성 존재. 이러하 이유로 경사하강법을 진행할수록 $\eta$의 값을 점진적으로 

### STOCHASTIC APPROXIMATION TO GRADIENT DESCENT
- 경사하강법의 문제점
1. Global minimum으로 수렴하는 것이 때때로 상당히 느릴 수 있음
2. Local minimum이 여러 개 있는 경우 global minimum으로 도달하지 않을 수도 있음
- 확률적 경사 하강법 사용
- 기본 경사하강법은 D의 모든 훈련 예제를 합산한 후 가중치를 update
- 확률적 경사 하강법은 각 개별 에제에 대한 오류 계산에 따라 가중치를 점진적으로 update
- Random하게 추출한 일부 data를 사용(전체 data 사용 X)
- 학습 중간 과정에서 결과의 진폭이 크고 불안정하며, 속도가 매우 빠름, memory 소모량 낮음
- 지역 최솟값에 빠진다할지라도, 지역 최솟값에 쉽게 빠져나올 수 있음

## MULTILAYER NETWORKS AND THE BACKPROPAGATION ALGORITHM
- 단일 퍼셉트론은 선형 결정 표면만을 표현 가능
- MLP는 다양한 비선형 결정 표면을 표현 가능

### A Differentiable Threshold Unit
- Sigmoid 단위 perceptron

![render](https://user-images.githubusercontent.com/80622859/189907873-1b7b6f6d-42ee-49dc-be6a-15f0260aad1e.png)

- 출력 범위는 0과 1 사이로 입력과 함께 단조롭게 증가
- 매우 큰 입력 domain을 작은 범위의 출력에 mapping하기 때문에 squashing function이라고도 함
- Sigmoid 함수 대신 tanh 함수가 사용되기도 함

### The Backpropagation Algorithm
- 역전파가 직면한 학습 문제는 network의 모든 unit에 대해 가능한 모든 가중치 값으로 정의된 큰 가설 공간을 검색하는 것
- 다층 네트워크의 경우에는 단일 최소 포물선 오류 표면과 달리 여러 개의 지역 최소값을 가질 수 있음

역전파(학습 data, 학습율, 입력값의 수, 은닉층의 unit 수, 출력값의 수):

    학습 data는 <input vector, target vector>의 형태를 뜀
    
    매개변수로 받은 입력값의 수와 은닉층의 unit 수 그리고 출력값의 수로 순전파 신경망 생성
    
    신경망의 모든 가중치를 -0.05~0.05 사이의 수로 무작위로 초기화
    
    종료 조건이 만족될 때까지 아래의 과정을 진행
    
        각각의 <input vector, tartget vector>에서 아래와 같은 과정을 진행
        
            신경망 계산
            
            1. input vector를 신경망에 입력하여 모든 u에 대해서 출력값(o_u) 계산
            
            역전파
            
            2.  각각의 출력값 k에 대해서 error term을 계산
            
            3. 각각의 은닉 단위 h에 대해서 error term 계산
            
            4. 가중치 update
            
            
          

#### 역전파 수식

<img width="114" alt="캡처" src="https://user-images.githubusercontent.com/80622859/194011011-6700d7a2-05bd-4446-87af-c188d5be0236.PNG">

- 연쇄 법칙을 따르면 위의 식이 성립
- 우변에 첫번째 항은 아래와 같음

<img width="168" alt="캡처" src="https://user-images.githubusercontent.com/80622859/194011205-51869616-e3e8-41cd-9c6c-e137dd33a6ac.PNG">

- Sigmoid의 미분값을 활용하여 2번쨰 항을 계산 시 아래와 같음

<img width="118" alt="캡처" src="https://user-images.githubusercontent.com/80622859/194011379-a3957940-c1c9-4236-9276-0ea77b378719.PNG">

- 위의 두 식을 곱하면 아래와 같음

<img width="168" alt="캡처" src="https://user-images.githubusercontent.com/80622859/194011448-7f82835d-1d5f-4702-83b9-5c1c9b1afef2.PNG">

- 최종 update 해야할 값은 아래와 같음

<img width="244" alt="캡처" src="https://user-images.githubusercontent.com/80622859/194011526-88868eb3-24d9-4230-aac0-1442cb6bda84.PNG">





#### 역전파 1단계

<img width="291" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910521-52862ed2-f002-4dfb-b741-4642f77c7c11.PNG">

- 순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치 update
- 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 update하는 단계를 역전파 1단계, 그리고 N층과 N층의 이전층 사이의 가중치를 update하는 단계를 역전파 2단계

<img width="116" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189909303-2ba6b96b-b92a-4a88-a786-c5c0344a4db0.PNG">

- Weights we have to update : $w_5\,,w_6\,,w_7\,,w_8$(4개)
- For updat $w_5$, we must calculate $\frac{\partial E_{total}}{\partial w_5}$
- 미분의 연쇄법칙

<img width="175" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910203-2bc0d63f-94c2-414e-baaa-62e542071121.PNG">

- $E_{total}$ : 순전파를 진행하고 계산했던 전체 오차값

<img width="337" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910367-c92f28e3-be9d-4ec1-a771-643e8b6b092c.PNG">

<img width="403" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910428-e4ce9f58-5c65-4972-9e81-1f0c6e7d8029.PNG">

- 두 번째 항
- Sigmoid 함수의 미분 = $f(x) \times (1-f(x))$

<img width="366" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910801-d8a2fcb0-0072-44e5-9bc2-eb5cb754b5bd.PNG">

- 세번째 항

<img width="152" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911071-321ad747-dc5c-456a-a88c-9e9fb0adc201.PNG">

- 최종

<img width="346" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911480-b10746ed-95fc-413d-84c9-6376a7587c2c.PNG">

<img width="347" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911553-c94f336b-2cd7-4b7a-9fa4-925debed080d.PNG">

#### 역전파 2단계

<img width="297" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911692-833c97d1-0270-4058-93bf-c20828b9e031.PNG">

- $w_1$ update = get $\frac{\partial E_{total}}{\partial w_1}

<img width="184" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911919-0a7bb19f-f17d-4f8c-8491-a72183cf0ff8.PNG">

<img width="145" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912041-b7dcd300-e16e-4793-9b30-9429dec30a29.PNG">

<img width="272" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912137-addc849b-9b9a-4d9e-a902-668704388e74.PNG">

<img width="324" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912265-e6117b18-7028-40cd-930b-38b3f5bdbea3.PNG">

- 첫번째 항을 구함
- 나머지 두 항

<img width="368" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912380-ea3789f8-86ea-44d1-b4fd-2fbf031ba2ab.PNG">

- 최종

<img width="310" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912475-e3b92a01-b147-4c00-b97b-ef43dd8aa313.PNG">

<img width="341" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912533-1bf66dc8-efe1-466b-9b81-909aa3520b3b.PNG">

### Adding Momentum
- n번째 반복의 가중치 update가 (n-1)번째 반복동안 발생한 update에 부분적으로 의존

![캡처](https://user-images.githubusercontent.com/80622859/189938457-a3ef441a-1e9b-4a36-b04b-374485780739.PNG)

- $\delta w_{ji} (n)$ : n번째 반복 동안 수행된 가중치 update, $\alpha$ : 0과 1 사이 운동량이라는 상수
- 우변의 두 번째 항은 운동량 항(momentum term)
- $\alpha$의 효과 : 한 반복에서 다음 반복까지 공이 같은 방향으로 굴러가도록 하는 경향이 있는 운동량을 추가
- Local minimum에 도달하여 운동량이 없을 경우 평평한 영역을 따라 공이 계속 굴러가도록 할 수 있음
- 기울기가 변하지 않은 영역에서 탐색의 단계 크기를 점진적으로 증가시켜 수렴 속도를 높이는 효과

### Learning in arbitary acyclic networks
- 가중치를 update하는 규칙은 유지. 변하는 값은 delta 값 계산 절차
- 
#### Directed Acyclic Graph(DAG)
- 순환을 가지지 않는 방향 그래프

![다운로드](https://user-images.githubusercontent.com/80622859/189940042-6336606f-a6cb-4bb3-9469-969c513a1e2d.png)

# Remarks on the Backpropagation Algorithm

### Convergence and Local Minima
- 경사하강법을 통하여 E를 반복적으로 줄임
- Multi Layer Network에서는 local mimnima가 많아 경사 하강을 하다가 갇힐 수 있음 = 반드시 global minimum으로 수렴하지 않음
- E는 반드시 global minimum일 필요는 없음
- Local minimum proble은 심각하게 발견되지 않음
- 초기 가중치가 0에 가까운 값일 경우 초기 경사하강 동안 network는 입력에서 거의 선형인 매우 부드러운 기능을 보여줌
- 이는 가중치가 0에 가까울 때 sigmoid 함수의 임계값 자체가 선형이기 때문
- 시간이 지나야 비선형적인 기능 가능
- Common heuristics to attempt to alleviate the problem
1. Add momentum
2. More use SGD then GD
3. 동일한 data를 사용하여 여러 net을 훈련시키되 각 net이 서로 다른 무작위 가중치로 초기화

### Representational Power of Feedforward Networks
- Boolean Function
- 연속 기능
- 임의 함수
- Feedforward network는 역전파를 위한 매우 표현적인 가설 

### Hypothesis Space Search and Inductive Bias
- 가설 공간은 n개의 network 가중치의 n차원 유클리드 공간 <-> Decision Tree 및 이산 표현을 기반으로 하는 다른 방법의 가설 공간
- E가 연속적 -> 미분 가능
- Data points 간의 원활한 보간

### Hidden Layer Representations
- Network 내부의 숨겨진 단위 계층에서 유용한 중간 표현을 발견할 수 있음

![캡처](https://user-images.githubusercontent.com/80622859/189942573-5ca769be-6548-4404-99fe-6259f817b4bb.PNG)

- Hidden layer에서 유용한 표현을 자동으로 발견하는 Multi layer network의 능력은 ANN의 방법의 특징
- 학습자가 인간 설계자가 명시적으로 도입하지 않은 기능을 발명할 수 있는 중요한 수준의 유연성 제공

### Generalization, Overfitting, and Stopping Criterion
- 가중치 update를 종료하기 위한 조건?
- 훈련 예제의 오류가 미리 결정된 값 아래로 떨어질 때까지 훈련을 계속하는 것
- 하지만 위는 좋지 않은 전략
- 역전파는 보이지 않는 다른 예에 비해 일반화 정확도를 감소시키는 대가로 훈련 예제를 과적합
- Epoch에 따라 어떻게 변화하는지 고려하여야

![캡처](https://user-images.githubusercontent.com/80622859/189943407-d060b699-ff07-49ae-8bba-9f57a311b30d.PNG)

- 과적합을 해결 하기 위한 방법
1. 각 반복 동안 각 가중치를 작은 요인만큼 줄이는 것(penalty) -> 복잡한 의사 결정 표면에 대해 학습을 편향
2. Validation dataset
3. 교차 검증

- Data가 적은 경우 과적합 해결하기 힘듦 -> K-fold cross validation

# Advanced Topics in artificial Neural Network

### Alternative Error Functions
- Penalty term

![캡처](https://user-images.githubusercontent.com/80622859/189944261-d911a510-55b5-4aaf-af6e-552fd8bb0ce2.PNG)

- 기울기의 오류 또는 target function의 도함수에 대한 항 추가
- 목표값과 관련하여 network의 cross entropy를 최소화

![캡처](https://user-images.githubusercontent.com/80622859/189944479-3cad2633-0350-4354-a7ee-301dc7c04219.PNG)

### Alternative Error Minimization Procedures
- 경사하강법은 가장 효율적인 방법은 아님
- 가중치 최적화 알고리즘
- 현재 가중치 벡터를 변경할 방향을 선택하는 것과 이동할 거리를 선택하는 두 가지 결정을 포함하는 것

#### Line search
- Update 방향을 지정하는 line을 선택하면 이 line을 따라 오류 함수의 최소값을 찾아 update 거리가 선택

![캡처](https://user-images.githubusercontent.com/80622859/189945017-dab526d3-240f-425c-8b4e-e255471bdd2e.PNG)

- 초기 추측값 $x^{(0)}$에서 시작하여 반복을 통하여 일련의 중간 단계의 해 $x^{(k)}$를 구하며 점진적으로 최적해에 접근 
- 다음 단계의 해 $x^{(k+1)}$는 현 단계 해 $x^{k}$에서 일정 step $\Delta x^k$로 일정한 step size $\eta^k$만큼 이동시킴

![캡처](https://user-images.githubusercontent.com/80622859/189945617-b4a62125-dfc8-47ff-befa-afdacf95822a.PNG)

- $\Delta x^k$ : 탐색 방향(search direction)
- Line search는 step size를 정하는 방법

![캡처](https://user-images.githubusercontent.com/80622859/189945779-8620f91b-d746-4d49-ade2-93370d49f563.PNG)

### Recurrent Networks
- 순환 신경망은 시계열에 적용되고 시간 t+1의 다른 장치에 대한 입력으로 시간 t의 network unit의 출력을 사용하는 인공 신경망
- 한계 : y(t+1)의 예측이 x(t)에 의존하며 x의 초기 값에 대한 y(t+1)의 가능한 의존성을 포착할 수 없음

![캡처](https://user-images.githubusercontent.com/80622859/189946242-c6c3ecf5-81a2-4264-93f1-76100e7ab74a.PNG)

- (b)의 그림을 통해 한계점을 극복
- Hidden layer에 새로운 단위 b와 새로운 입력 단위 c(t)를 추가. c(t)는 시간 t-1에서 단위 b의 값
- 한 단계에서 네트워크에 대한 입력값 c(t)는 이전 단계에서의 단위 b의 값에서 복사
- b는 시간적으로 먼 x의 초기 값으로부터 정보를 요약 가능

### Dynamically Modifying Network Structure
- 지금까지 신경망 학습을 가중치를 조정하는 문제로 간주
- 일반화 정도와 훈련 효율성을 향상시키기 위해 network unit과 상호 연결의 수를 동적으로 증가시키거나 감소시키기 위한 다양한 방법 제안

#### Cascade-Correlation algorithm
- Hidden layer가 없는 net을 구성하는 것으로 시작
- 오류가 많다는 것을 발견 후 hidden layer 추가
- 각 단계에서 단 하나의 layer만 훈련
- 과적합 위험성

- 반대 방법으로 복잡한 신경망으로 시작하여서 특정 연결이 필수적이지 않다면 제거
